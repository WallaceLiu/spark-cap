{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0132ce60",
   "metadata": {},
   "source": [
    "# PySpark Shared Variables Tutorial\n",
    "In this section of the PySpark RDD tutorial, let’s learn what are the different types of PySpark Shared variables and how they are used in PySpark transformations.\n",
    "\n",
    "When PySpark executes transformation using map() or reduce() operations, It executes the transformations on a remote node by using the variables that are shipped with the tasks and these variables are not sent back to PySpark Driver hence there is no capability to reuse and sharing the variables across tasks. PySpark shared variables solve this problem using the below two techniques. PySpark provides two types of shared variables.\n",
    "\n",
    "- Broadcast variables (read-only shared variable)\n",
    "- Accumulator variables (updatable shared variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0e0a99",
   "metadata": {},
   "source": [
    "## Broadcast read-only Variables\n",
    "Broadcast variables are read-only shared variables that are cached and available on all nodes in a cluster in-order to access or use by the tasks. Instead of sending this data along with every task, PySpark distributes broadcast variables to the machine using efficient broadcast algorithms to reduce communication costs.\n",
    "\n",
    "One of the best use-case of PySpark RDD Broadcast is to use with lookup data for example zip code, state, country lookups e.t.c\n",
    "\n",
    "When you run a PySpark RDD job that has the Broadcast variables defined and used, PySpark does the following."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512c4f6e",
   "metadata": {},
   "source": [
    "- PySpark breaks the job into stages that have distributed shuffling and actions are executed with in the stage.\n",
    "- Later Stages are also broken into tasks\n",
    "- PySpark broadcasts the common data (reusable) needed by tasks within each stage.\n",
    "- The broadcasted data is cache in serialized format and deserialized before executing each task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb9e656",
   "metadata": {},
   "source": [
    "The PySpark Broadcast is created using the broadcast(v) method of the SparkContext class. This method takes the argument v that you want to broadcast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "027ed497",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "broadcastVar = sc.broadcast([0, 1, 2, 3])\n",
    "broadcastVar.value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d42747f",
   "metadata": {},
   "source": [
    "Note that broadcast variables are not sent to executors with sc.broadcast(variable) call instead, they will be sent to executors when they are first used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304d384c",
   "metadata": {},
   "source": [
    "[更多参考](https://sparkbyexamples.com/pyspark/pyspark-broadcast-variables/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccaf586b",
   "metadata": {},
   "source": [
    "## Accumulators\n",
    "PySpark Accumulators are another type shared variable that are only “added” through an associative and commutative operation and are used to perform counters (Similar to Map-reduce counters) or sum operations.\n",
    "\n",
    "PySpark by default supports creating an accumulator of any numeric type and provides the capability to add custom accumulator types. Programmers can create following accumulators"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88ddc5b",
   "metadata": {},
   "source": [
    "- named accumulators\n",
    "- unnamed accumulators"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f80a6e8",
   "metadata": {},
   "source": [
    "When you create a named accumulator, you can see them on PySpark web UI under the “Accumulator” tab. On this tab, you will see two tables; the first table “accumulable” – consists of all named accumulator variables and their values. And on the second table “Tasks” – value for each accumulator modified by a task.\n",
    "\n",
    "Where as unnamed accumulators are not shows on PySpark web UI, For all practical purposes it is suggestable to use named accumulators.\n",
    "\n",
    "Accumulator variables are created using `SparkContext.longAccumulator(v)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "704a4750",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.3.15:4043\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=PySparkShell>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eacfd8db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "accum = sc.accumulator(0)\n",
    "s = sc.parallelize([1, 2, 3]).foreach(lambda x: accum.add(x))\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23dad99e",
   "metadata": {},
   "source": [
    "- [Long Accumulator](https://sparkbyexamples.com/spark/spark-accumulators/#LongAccumulator)\n",
    "- [Double Accumulator](https://sparkbyexamples.com/spark/spark-accumulators/#DoubleAccumulator)\n",
    "- [Collection Accumulator](https://sparkbyexamples.com/spark/spark-accumulators/#CollectionAccumulator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49dbf0d",
   "metadata": {},
   "source": [
    "# example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "efcb3a59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('James', 'Smith', 'USA', 'California'), ('Michael', 'Rose', 'USA', 'New York'), ('Robert', 'Williams', 'USA', 'California'), ('Maria', 'Jones', 'USA', 'Florida')]\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "\n",
    "states = {\"NY\": \"New York\", \"CA\": \"California\", \"FL\": \"Florida\"}\n",
    "broadcastStates = spark.sparkContext.broadcast(states)\n",
    "\n",
    "data = [(\"James\", \"Smith\", \"USA\", \"CA\"), (\"Michael\", \"Rose\", \"USA\", \"NY\"),\n",
    "        (\"Robert\", \"Williams\", \"USA\", \"CA\"), (\"Maria\", \"Jones\", \"USA\", \"FL\")]\n",
    "\n",
    "rdd = spark.sparkContext.parallelize(data)\n",
    "\n",
    "def state_convert(code):\n",
    "    return broadcastStates.value[code]\n",
    "\n",
    "\n",
    "result = rdd.map(lambda x: (x[0], x[1], x[2], state_convert(x[3]))).collect()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499dd07b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
