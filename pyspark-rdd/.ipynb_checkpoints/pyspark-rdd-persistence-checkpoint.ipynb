{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99fea08f",
   "metadata": {},
   "source": [
    "# PySpark RDD Persistence Tutorial\n",
    "PySpark Cache and Persist are optimization techniques to improve the performance of the RDD jobs that are iterative and interactive. In this PySpark RDD Tutorial section, I will explain how to use persist() and cache() methods on RDD with examples.\n",
    "\n",
    "Though PySpark provides computation 100 x times faster than traditional Map Reduce jobs, If you have not designed the jobs to reuse the repeating computations you will see degrade in performance when you are dealing with billions or trillions of data. Hence, we need to look at the computations and use optimization techniques as one of the ways to improve performance.\n",
    "\n",
    "Using cache() and persist() methods, PySpark provides an optimization mechanism to store the intermediate computation of an RDD so they can be reused in subsequent actions.\n",
    "\n",
    "When you persist or cache an RDD, each worker node stores it’s partitioned data in memory or disk and reuses them in other actions on that RDD. And Spark’s persisted data on nodes are fault-tolerant meaning if any partition is lost, it will automatically be recomputed using the original transformations that created it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4f36b7",
   "metadata": {},
   "source": [
    "## Advantages of Persisting RDD\n",
    "- Cost efficient – PySpark computations are very expensive hence reusing the computations are used to save cost.\n",
    "- Time efficient – Reusing the repeated computations saves lots of time.\n",
    "- Execution time – Saves execution time of the job which allows us to perform more jobs on the same cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9ceea9",
   "metadata": {},
   "source": [
    "### RDD Cache\n",
    "PySpark RDD <strong>cache()</strong> method by default saves RDD computation to storage level `<strong>MEMORY_ONLY</strong>` meaning it will store the data in the JVM heap as unserialized objects.\n",
    "\n",
    "PySpark cache() method in RDD class internally calls persist() method which in turn uses sparkSession.sharedState.cacheManager.cacheQuery to cache the result set of RDD. Let’s look at an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ccc547cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
    "rdd = spark.sparkContext.parallelize(data)\n",
    "cachedRdd = rdd.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3a7b486",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[1] at readRDDFromFile at PythonRDD.scala:274"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cachedRdd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11dced8",
   "metadata": {},
   "source": [
    "### RDD Persist\n",
    "PySpark persist() method is used to store the RDD to one of the storage levels <strong>MEMORY_ONLY</strong>,<strong>MEMORY_AND_DISK</strong>, <strong>MEMORY_ONLY_SER</strong>, <strong>MEMORY_AND_DISK_SER</strong>, <strong>DISK_ONLY</strong>, <strong>MEMORY_ONLY_2</strong>,<strong>MEMORY_AND_DISK_2</strong> and more.\n",
    "\n",
    "PySpark persist has two signature first signature doesn’t take any argument which by default saves it to <strong>MEMORY_ONLY</strong> storage level and the second signature which takes StorageLevel as an argument to store it to different storage levels.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da688403",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[1] at readRDDFromFile at PythonRDD.scala:274"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark\n",
    "rddPersist = rdd.persist(pyspark.StorageLevel.MEMORY_ONLY)\n",
    "rddPersist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b08480e",
   "metadata": {},
   "source": [
    "### RDD Unpersist\n",
    "PySpark automatically monitors every persist() and cache() calls you make and it checks usage on each node and drops persisted data if not used or by using least-recently-used (LRU) algorithm. You can also manually remove using unpersist() method. unpersist() marks the RDD as non-persistent, and remove all blocks for it from memory and disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bdeb80ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "rddPersist2 = rddPersist.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36adb39a",
   "metadata": {},
   "source": [
    "unpersist(Boolean) with boolean as argument blocks until all blocks are deleted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7986223a",
   "metadata": {},
   "source": [
    "## Persistence Storage Levels\n",
    "All different storage level PySpark supports are available at org.apache.spark.storage.StorageLevel class. Storage Level defines how and where to store the RDD.\n",
    "\n",
    "<strong>MEMORY_ONLY</strong> – This is the default behavior of the RDD cache() method and stores the RDD as deserialized objects to JVM memory. When there is no enough memory available it will not save to RDD of some partitions and these will be re-computed as and when required. This takes more storage but runs faster as it takes few CPU cycles to read from memory.\n",
    "\n",
    "<strong>MEMORY_ONLY_SER</strong> – This is the same as MEMORY_ONLY but the difference being it stores RDD as serialized objects to JVM memory. It takes lesser memory (space-efficient) then MEMORY_ONLY as it saves objects as serialized and takes an additional few more CPU cycles in order to deserialize.\n",
    "\n",
    "<strong>MEMORY_ONLY_2</strong> – Same as MEMORY_ONLY storage level but replicate each partition to two cluster nodes.\n",
    "\n",
    "<strong>MEMORY_ONLY_SER_2</strong> – Same as MEMORY_ONLY_SER storage level but replicate each partition to two cluster nodes.\n",
    "\n",
    "<strong>MEMORY_AND_DISK</strong> – In this Storage Level, The RDD will be stored in JVM memory as a deserialized objects. When required storage is greater than available memory, it stores some of the excess partitions in to disk and reads the data from disk when it required. It is slower as there is I/O involved.\n",
    "\n",
    "<strong>MEMORY_AND_DISK_SER</strong> – This is same as MEMORY_AND_DISK storage level difference being it serializes the RDD objects in memory and on disk when space not available.\n",
    "\n",
    "<strong>MEMORY_AND_DISK_2</strong> – Same as MEMORY_AND_DISK storage level but replicate each partition to two cluster nodes.\n",
    "\n",
    "<strong>MEMORY_AND_DISK_SER_2</strong> – Same as MEMORY_AND_DISK_SER storage level but replicate each partition to two cluster nodes.\n",
    "\n",
    "<strong>DISK_ONLY</strong> – In this storage level, RDD is stored only on disk and the CPU computation time is high as I/O involved.\n",
    "\n",
    "<strong>DISK_ONLY_2</strong> – Same as DISK_ONLY storage level but replicate each partition to two cluster nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca328fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
