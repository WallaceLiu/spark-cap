{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd32bda1",
   "metadata": {},
   "source": [
    "# pandas DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ea427bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Name  Age\n",
      "0   Scott   50\n",
      "1    Jeff   45\n",
      "2  Thomas   54\n",
      "3     Ann   34\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = [['Scott', 50], ['Jeff', 45], ['Thomas', 54], ['Ann', 34]]\n",
    "\n",
    "# Create the pandas DataFrame\n",
    "pandasDF = pd.DataFrame(data, columns=['Name', 'Age'])\n",
    "\n",
    "# print dataframe.\n",
    "print(pandasDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f76a782",
   "metadata": {},
   "source": [
    "# pandas DF 转换成 spark DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d67e3a4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Age: long (nullable = true)\n",
      "\n",
      "+------+---+\n",
      "|  Name|Age|\n",
      "+------+---+\n",
      "| Scott| 50|\n",
      "|  Jeff| 45|\n",
      "|Thomas| 54|\n",
      "|   Ann| 34|\n",
      "+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sparkDF = spark.createDataFrame(pandasDF)\n",
    "sparkDF.printSchema()\n",
    "sparkDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a191ceb",
   "metadata": {},
   "source": [
    "# pandas DF 转换成 spark DF，并且指定列名"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86a07fc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- First Name: string (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      "\n",
      "+----------+---+\n",
      "|First Name|Age|\n",
      "+----------+---+\n",
      "|     Scott| 50|\n",
      "|      Jeff| 45|\n",
      "|    Thomas| 54|\n",
      "|       Ann| 34|\n",
      "+----------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#sparkDF=spark.createDataFrame(pandasDF.astype(str))\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "mySchema = StructType([ StructField(\"First Name\", StringType(), True)\\\n",
    "                       ,StructField(\"Age\", IntegerType(), True)])\n",
    "\n",
    "sparkDF2 = spark.createDataFrame(pandasDF, schema=mySchema)\n",
    "sparkDF2.printSchema()\n",
    "sparkDF2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38fd3da3",
   "metadata": {},
   "source": [
    "# spark DF 转换成 pandas DF，并且使用pyarrow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0609c3a",
   "metadata": {},
   "source": [
    "[pyarrow](https://spark.apache.org/docs/3.0.1/sql-pyspark-pandas-with-arrow.html)\n",
    "\n",
    "Apache Arrow in PySpark\n",
    "Apache Arrow is an in-memory columnar data format that is used in Spark to efficiently transfer data between JVM and Python processes. This currently is most beneficial to Python users that work with Pandas/NumPy data. Its usage is not automatic and might require some minor changes to configuration or code to take full advantage and ensure compatibility. This guide will give a high-level description of how to use Arrow in Spark and highlight any differences when working with Arrow-enabled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b6b21c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.fallback.enabled\", \"true\")\n",
    "\n",
    "pandasDF2 = sparkDF2.select(\"*\").toPandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d0c11301",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method PandasConversionMixin.toPandas of DataFrame[First Name: string, Age: int]>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pandasDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "34d71275",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'true'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.get(\"spark.sql.execution.arrow.enabled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f924fff3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'true'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.get(\"spark.sql.execution.arrow.pyspark.fallback.enabled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896147a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82a455d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
