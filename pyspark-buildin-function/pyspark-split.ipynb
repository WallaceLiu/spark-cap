{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f65a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "author SparkByExamples.com\n",
    "\"\"\"\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import split, col\n",
    "\n",
    "spark = SparkSession.builder.appName(\"sparkbyexamples\").getOrCreate()\n",
    "\n",
    "data = data = [('James', '', 'Smith', '1991-04-01'),\n",
    "               ('Michael', 'Rose', '', '2000-05-19'),\n",
    "               ('Robert', '', 'Williams', '1978-09-05'),\n",
    "               ('Maria', 'Anne', 'Jones', '1967-12-01'),\n",
    "               ('Jen', 'Mary', 'Brown', '1980-02-17')]\n",
    "\n",
    "columns = [\"firstname\", \"middlename\", \"lastname\", \"dob\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)\n",
    "df1 = df.withColumn('year', split(df['dob'], '-').getItem(0)) \\\n",
    "       .withColumn('month', split(df['dob'], '-').getItem(1)) \\\n",
    "       .withColumn('day', split(df['dob'], '-').getItem(2))\n",
    "df1.printSchema()\n",
    "df1.show(truncate=False)\n",
    "\n",
    "# Alternatively we can do like below\n",
    "split_col = pyspark.sql.functions.split(df['dob'], '-')\n",
    "df2 = df.withColumn('year', split_col.getItem(0)) \\\n",
    "       .withColumn('month', split_col.getItem(1)) \\\n",
    "       .withColumn('day', split_col.getItem(2))\n",
    "df2.show(truncate=False)\n",
    "\n",
    "# Using split() function of Column class\n",
    "split_col = pyspark.sql.functions.split(df['dob'], '-')\n",
    "df3 = df.select(\"firstname\", \"middlename\", \"lastname\", \"dob\",\n",
    "                split_col.getItem(0).alias('year'),\n",
    "                split_col.getItem(1).alias('month'),\n",
    "                split_col.getItem(2).alias('day'))\n",
    "df3.show(truncate=False)\n",
    "\"\"\"\n",
    "df4=spark.createDataFrame([(\"20-13-2012-monday\",)], ['date',])\n",
    "\n",
    "df4.select(split(df4.date,'^([\\d]+-[\\d]+-[\\d])').alias('date'),\n",
    "    regexp_replace(split(df4.date,'^([\\d]+-[\\d]+-[\\d]+)').getItem(1),'-','').alias('day')).show()\n",
    "    \"\"\"\n",
    "df4 = spark.createDataFrame([('oneAtwoBthree', )], [\n",
    "    'str',\n",
    "])\n",
    "df4.select(split(df4.str, '[AB]').alias('str')).show()\n",
    "\n",
    "df4.select(split(df4.str, '[AB]', 2).alias('str')).show()\n",
    "df4.select(split(df4.str, '[AB]', 1).alias('str')).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
