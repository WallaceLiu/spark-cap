{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c19767b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_name: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- bonus: long (nullable = true)\n",
      "\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|employee_name|department|state|salary|age|bonus|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|James        |Sales     |NY   |90000 |34 |10000|\n",
      "|Michael      |Sales     |NV   |86000 |56 |20000|\n",
      "|Robert       |Sales     |CA   |81000 |30 |23000|\n",
      "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
      "|Raman        |Finance   |DE   |99000 |40 |24000|\n",
      "|Scott        |Finance   |NY   |83000 |36 |19000|\n",
      "|Jen          |Finance   |NY   |79000 |53 |15000|\n",
      "|Jeff         |Marketing |NV   |80000 |25 |18000|\n",
      "|Kumar        |Marketing |NJ   |91000 |50 |21000|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum, avg, max\n",
    "\n",
    "simpleData = [(\"James\", \"Sales\", \"NY\", 90000, 34, 10000),\n",
    "              (\"Michael\", \"Sales\", \"NV\", 86000, 56, 20000),\n",
    "              (\"Robert\", \"Sales\", \"CA\", 81000, 30, 23000),\n",
    "              (\"Maria\", \"Finance\", \"CA\", 90000, 24, 23000),\n",
    "              (\"Raman\", \"Finance\", \"DE\", 99000, 40, 24000),\n",
    "              (\"Scott\", \"Finance\", \"NY\", 83000, 36, 19000),\n",
    "              (\"Jen\", \"Finance\", \"NY\", 79000, 53, 15000),\n",
    "              (\"Jeff\", \"Marketing\", \"NV\", 80000, 25, 18000),\n",
    "              (\"Kumar\", \"Marketing\", \"NJ\", 91000, 50, 21000)]\n",
    "\n",
    "schema = [\"employee_name\", \"department\", \"state\", \"salary\", \"age\", \"bonus\"]\n",
    "df = spark.createDataFrame(data=simpleData, schema=schema)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "335f4eab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+\n",
      "|state|sum(salary)|\n",
      "+-----+-----------+\n",
      "|   NJ|      91000|\n",
      "|   NV|     166000|\n",
      "|   CA|     171000|\n",
      "|   DE|      99000|\n",
      "|   NY|     252000|\n",
      "+-----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"state\").sum(\"salary\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "240767db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+\n",
      "|state|sum_salary|\n",
      "+-----+----------+\n",
      "|NJ   |91000     |\n",
      "|NV   |166000    |\n",
      "|CA   |171000    |\n",
      "|DE   |99000     |\n",
      "|NY   |252000    |\n",
      "+-----+----------+\n",
      "\n",
      "+-----+----------+\n",
      "|state|sum_salary|\n",
      "+-----+----------+\n",
      "|   NV|    166000|\n",
      "|   CA|    171000|\n",
      "|   NY|    252000|\n",
      "+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfGroup=df.groupBy(\"state\") \\\n",
    "          .agg(sum(\"salary\").alias(\"sum_salary\"))\n",
    "dfGroup.show(truncate=False)\n",
    "\n",
    "dfFilter = dfGroup.filter(dfGroup.sum_salary > 100000)\n",
    "dfFilter.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a38ac75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+\n",
      "|state|sum_salary|\n",
      "+-----+----------+\n",
      "|   NV|    166000|\n",
      "|   CA|    171000|\n",
      "|   NY|    252000|\n",
      "+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import asc\n",
    "\n",
    "dfFilter.sort(\"sum_salary\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "869e2245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+\n",
      "|state|sum_salary|\n",
      "+-----+----------+\n",
      "|   NY|    252000|\n",
      "|   CA|    171000|\n",
      "|   NV|    166000|\n",
      "+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import desc\n",
    "\n",
    "dfFilter.sort(desc(\"sum_salary\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f17ce6f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+\n",
      "|state|sum_salary|\n",
      "+-----+----------+\n",
      "|   NY|    252000|\n",
      "|   CA|    171000|\n",
      "|   NV|    166000|\n",
      "+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"state\") \\\n",
    "  .agg(sum(\"salary\").alias(\"sum_salary\")) \\\n",
    "  .filter(col(\"sum_salary\") > 100000)  \\\n",
    "  .sort(desc(\"sum_salary\")) \\\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0afe211f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+\n",
      "|state|sum_salary|\n",
      "+-----+----------+\n",
      "|   NY|    252000|\n",
      "|   CA|    171000|\n",
      "|   NV|    166000|\n",
      "+-----+----------+\n",
      "\n",
      "+-----+----------+\n",
      "|state|sum_salary|\n",
      "+-----+----------+\n",
      "|   NJ|     91000|\n",
      "|   NV|    166000|\n",
      "|   CA|    171000|\n",
      "|   DE|     99000|\n",
      "|   NY|    252000|\n",
      "+-----+----------+\n",
      "\n",
      "+-----+----------+\n",
      "|state|sum_salary|\n",
      "+-----+----------+\n",
      "|   NJ|     91000|\n",
      "|   NV|    166000|\n",
      "|   CA|    171000|\n",
      "|   DE|     99000|\n",
      "|   NY|    252000|\n",
      "+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView(\"EMP\")\n",
    "spark.sql(\"select state, sum(salary) as sum_salary from EMP \" +\n",
    "          \"group by state having sum_salary > 100000 \" +\n",
    "          \"order by sum_salary desc\").show()\n",
    "\n",
    "df.groupBy(\"state\") \\\n",
    "  .sum(\"salary\") \\\n",
    "  .withColumnRenamed(\"sum(salary)\", \"sum_salary\") \\\n",
    "  .show()\n",
    "\n",
    "df.groupBy(\"state\") \\\n",
    "  .sum(\"salary\") \\\n",
    "  .select(col(\"state\"),col(\"sum(salary)\").alias(\"sum_salary\")) \\\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ba5a54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
