{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6e806b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "               .appName('SparkByExamples.com') \\\n",
    "               .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b84fc8e",
   "metadata": {},
   "source": [
    "## date convert to string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4ad9a79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+----------------+------------+------------------+\n",
      "|current_date|yyyy MM dd|      MM/dd/yyyy|yyyy MMMM dd|    yyyy MMMM dd E|\n",
      "+------------+----------+----------------+------------+------------------+\n",
      "|  2021-08-16|2021 08 16|08/16/2021 11:07| 2021 Aug 16|2021 August 16 Mon|\n",
      "+------------+----------+----------------+------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([[\"1\"]], [\"id\"])\n",
    "df.select(current_date().alias(\"current_date\"), \\\n",
    "      date_format(current_date(),\"yyyy MM dd\").alias(\"yyyy MM dd\"), \\\n",
    "      date_format(current_timestamp(),\"MM/dd/yyyy hh:mm\").alias(\"MM/dd/yyyy\"), \\\n",
    "      date_format(current_timestamp(),\"yyyy MMM dd\").alias(\"yyyy MMMM dd\"), \\\n",
    "      date_format(current_timestamp(),\"yyyy MMMM dd E\").alias(\"yyyy MMMM dd E\") \\\n",
    "   ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38eb845",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SQL\n",
    "spark.sql(\n",
    "    \"select current_date() as current_date, \" +\n",
    "    \"date_format(current_timestamp(),'yyyy MM dd') as yyyy_MM_dd, \" +\n",
    "    \"date_format(current_timestamp(),'MM/dd/yyyy hh:mm') as MM_dd_yyyy, \" +\n",
    "    \"date_format(current_timestamp(),'yyyy MMM dd') as yyyy_MMMM_dd, \" +\n",
    "    \"date_format(current_timestamp(),'yyyy MMMM dd E') as yyyy_MMMM_dd_E\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb9d397",
   "metadata": {},
   "source": [
    "## string convert to date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05c805a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|     input|      date|\n",
      "+----------+----------+\n",
      "|02-03-2013|2013-02-03|\n",
      "|05-06-2023|2023-05-06|\n",
      "+----------+----------+\n",
      "\n",
      "+----------+\n",
      "|      date|\n",
      "+----------+\n",
      "|2013-02-03|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([[\"02-03-2013\"], [\"05-06-2023\"]], [\"input\"])\n",
    "df.select(col(\"input\"),to_date(col(\"input\"),\"MM-dd-yyyy\").alias(\"date\")) \\\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0bb8da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SQL\n",
    "spark.sql(\"select to_date('02-03-2013','MM-dd-yyyy') date\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c4fed8",
   "metadata": {},
   "source": [
    "## timestamp convert to date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "894f7df6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- input_timestamp: string (nullable = true)\n",
      "\n",
      "+---+-----------------------+----------+\n",
      "|id |input_timestamp        |date_type |\n",
      "+---+-----------------------+----------+\n",
      "|1  |2019-06-24 12:01:19.000|2019-06-24|\n",
      "+---+-----------------------+----------+\n",
      "\n",
      "+---+-----------------------+----------+\n",
      "|id |input_timestamp        |date_type |\n",
      "+---+-----------------------+----------+\n",
      "|1  |2019-06-24 12:01:19.000|2019-06-24|\n",
      "+---+-----------------------+----------+\n",
      "\n",
      "+----------------------------------------------------------+\n",
      "|to_date(06-24-2019 12:01:19.000, MM-dd-yyyy HH:mm:ss.SSSS)|\n",
      "+----------------------------------------------------------+\n",
      "|                                                2019-06-24|\n",
      "+----------------------------------------------------------+\n",
      "\n",
      "+---+-----------------------+----------+\n",
      "|id |input_timestamp        |date_type |\n",
      "+---+-----------------------+----------+\n",
      "|1  |2019-06-24 12:01:19.000|2019-06-24|\n",
      "+---+-----------------------+----------+\n",
      "\n",
      "+---+-----------------------+----------+\n",
      "|id |input_timestamp        |date_type |\n",
      "+---+-----------------------+----------+\n",
      "|1  |2019-06-24 12:01:19.000|2021-08-16|\n",
      "+---+-----------------------+----------+\n",
      "\n",
      "+---+-----------------------+-------------------+----------+\n",
      "|id |input_timestamp        |ts                 |datetype  |\n",
      "+---+-----------------------+-------------------+----------+\n",
      "|1  |2019-06-24 12:01:19.000|2019-06-24 12:01:19|2019-06-24|\n",
      "+---+-----------------------+-------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame(data=[(\"1\", \"2019-06-24 12:01:19.000\")],\n",
    "                           schema=[\"id\", \"input_timestamp\"])\n",
    "df.printSchema()\n",
    "\n",
    "# Using Cast to convert Timestamp String to DateType\n",
    "df.withColumn('date_type', col('input_timestamp').cast('date')) \\\n",
    "       .show(truncate=False)\n",
    "\n",
    "# Using Cast to convert TimestampType to DateType\n",
    "df.withColumn('date_type', to_timestamp('input_timestamp').cast('date')) \\\n",
    "  .show(truncate=False)\n",
    "\n",
    "df.select(to_date(lit('06-24-2019 12:01:19.000'),'MM-dd-yyyy HH:mm:ss.SSSS')) \\\n",
    "  .show()\n",
    "\n",
    "#Timestamp String to DateType\n",
    "df.withColumn(\"date_type\",to_date(\"input_timestamp\")) \\\n",
    "  .show(truncate=False)\n",
    "\n",
    "#Timestamp Type to DateType\n",
    "df.withColumn(\"date_type\",to_date(current_timestamp())) \\\n",
    "  .show(truncate=False)\n",
    "\n",
    "df.withColumn(\"ts\",to_timestamp(col(\"input_timestamp\"))) \\\n",
    "  .withColumn(\"datetype\",to_date(col(\"ts\"))) \\\n",
    "  .show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8cd2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SQL TimestampType to DateType\n",
    "spark.sql(\"select to_date(current_timestamp) as date_type\")\n",
    "#SQL CAST TimestampType to DateType\n",
    "spark.sql(\"select date(to_timestamp('2019-06-24 12:01:19.000')) as date_type\")\n",
    "#SQL CAST timestamp string to DateType\n",
    "spark.sql(\"select date('2019-06-24 12:01:19.000') as date_type\")\n",
    "#SQL Timestamp String (default format) to DateType\n",
    "spark.sql(\"select to_date('2019-06-24 12:01:19.000') as date_type\")\n",
    "#SQL Custom Timeformat to DateType\n",
    "spark.sql(\n",
    "    \"select to_date('06-24-2019 12:01:19.000','MM-dd-yyyy HH:mm:ss.SSSS') as date_type\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278b3fcf",
   "metadata": {},
   "source": [
    "## string convert to timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "01b73c5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- input_timestamp: string (nullable = true)\n",
      "\n",
      "+---+-----------------------+-------------------+\n",
      "|id |input_timestamp        |timestamp          |\n",
      "+---+-----------------------+-------------------+\n",
      "|1  |2019-06-24 12:01:19.000|2019-06-24 12:01:19|\n",
      "+---+-----------------------+-------------------+\n",
      "\n",
      "+---+-----------------------+-------------------+\n",
      "|id |input_timestamp        |timestamp          |\n",
      "+---+-----------------------+-------------------+\n",
      "|1  |2019-06-24 12:01:19.000|2019-06-24 12:01:19|\n",
      "+---+-----------------------+-------------------+\n",
      "\n",
      "+---------------------------------------------------------------+\n",
      "|to_timestamp(06-24-2019 12:01:19.000, MM-dd-yyyy HH:mm:ss.SSSS)|\n",
      "+---------------------------------------------------------------+\n",
      "|2019-06-24 12:01:19                                            |\n",
      "+---------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame(data=[(\"1\", \"2019-06-24 12:01:19.000\")],\n",
    "                           schema=[\"id\", \"input_timestamp\"])\n",
    "df.printSchema()\n",
    "\n",
    "#Timestamp String to DateType\n",
    "df.withColumn(\"timestamp\",to_timestamp(\"input_timestamp\")) \\\n",
    "  .show(truncate=False)\n",
    "\n",
    "# Using Cast to convert TimestampType to DateType\n",
    "df.withColumn('timestamp', \\\n",
    "         to_timestamp('input_timestamp').cast('string')) \\\n",
    "  .show(truncate=False)\n",
    "\n",
    "\n",
    "df.select(to_timestamp(lit('06-24-2019 12:01:19.000'),'MM-dd-yyyy HH:mm:ss.SSSS')) \\\n",
    "  .show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12746c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SQL string to TimestampType\n",
    "spark.sql(\"select to_timestamp('2019-06-24 12:01:19.000') as timestamp\")\n",
    "#SQL CAST timestamp string to TimestampType\n",
    "spark.sql(\"select timestamp('2019-06-24 12:01:19.000') as timestamp\")\n",
    "#SQL Custom string to TimestampType\n",
    "spark.sql(\n",
    "    \"select to_timestamp('06-24-2019 12:01:19.000','MM-dd-yyyy HH:mm:ss.SSSS') as timestamp\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e96f3a",
   "metadata": {},
   "source": [
    "## current-date conver  to timestamp.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f509d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[\"1\"]]\n",
    "df = spark.createDataFrame(data, [\"id\"])\n",
    "\n",
    "#current_date() & current_timestamp()\n",
    "df.withColumn(\"current_date\",current_date()) \\\n",
    "  .withColumn(\"current_timestamp\",current_timestamp()) \\\n",
    "  .show(truncate=False)\n",
    "\n",
    "#SQL\n",
    "spark.sql(\"select current_date(), current_timestamp()\") \\\n",
    "     .show(truncate=False)\n",
    "\n",
    "# Date & Timestamp into custom format\n",
    "df.withColumn(\"date_format\",date_format(current_date(),\"MM-dd-yyyy\")) \\\n",
    "  .withColumn(\"to_timestamp\",to_timestamp(current_timestamp(),\"MM-dd-yyyy HH mm ss SSS\")) \\\n",
    "  .show(truncate=False)\n",
    "\n",
    "#SQL\n",
    "spark.sql(\"select date_format(current_date(),'MM-dd-yyyy') as date_format ,\" + \\\n",
    "          \"to_timestamp(current_timestamp(),'MM-dd-yyyy HH mm ss SSS') as to_timestamp\") \\\n",
    "     .show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8353169",
   "metadata": {},
   "source": [
    "## datediff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e03d960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+--------+\n",
      "|      date|current_date|datediff|\n",
      "+----------+------------+--------+\n",
      "|2019-07-01|  2021-08-16|     777|\n",
      "|2019-06-24|  2021-08-16|     784|\n",
      "|2019-08-24|  2021-08-16|     723|\n",
      "+----------+------------+--------+\n",
      "\n",
      "+---+----------+---------+-----------+---------------+------------------+---------------+\n",
      "| id|      date|datesDiff|  montsDiff|montsDiff_round|         yearsDiff|yearsDiff_round|\n",
      "+---+----------+---------+-----------+---------------+------------------+---------------+\n",
      "|  1|2019-07-01|      777|25.48387097|          25.48|2.1236559141666667|           2.12|\n",
      "|  2|2019-06-24|      784|25.74193548|          25.74|        2.14516129|           2.15|\n",
      "|  3|2019-08-24|      723|23.74193548|          23.74|1.9784946233333331|           1.98|\n",
      "+---+----------+---------+-----------+---------------+------------------+---------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[date: date, endDate: date]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [(\"1\", \"2019-07-01\"), (\"2\", \"2019-06-24\"), (\"3\", \"2019-08-24\")]\n",
    "\n",
    "df = spark.createDataFrame(data=data, schema=[\"id\", \"date\"])\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "df.select(col(\"date\"),\n",
    "          current_date().alias(\"current_date\"),\n",
    "          datediff(current_date(), col(\"date\")).alias(\"datediff\")).show()\n",
    "\n",
    "df.withColumn(\"datesDiff\", datediff(current_date(),col(\"date\"))) \\\n",
    "  .withColumn(\"montsDiff\", months_between(current_date(),col(\"date\"))) \\\n",
    "  .withColumn(\"montsDiff_round\",round(months_between(current_date(),col(\"date\")),2)) \\\n",
    "  .withColumn(\"yearsDiff\",months_between(current_date(),col(\"date\"))/lit(12)) \\\n",
    "  .withColumn(\"yearsDiff_round\",round(months_between(current_date(),col(\"date\"))/lit(12),2)) \\\n",
    "  .show()\n",
    "\n",
    "data2 = [(\"1\", \"07-01-2019\"), (\"2\", \"06-24-2019\"), (\"3\", \"08-24-2019\")]\n",
    "df2 = spark.createDataFrame(data=data2, schema=[\"id\", \"date\"])\n",
    "df2.select(\n",
    "    to_date(col(\"date\"), \"MM-dd-yyyy\").alias(\"date\"),\n",
    "    current_date().alias(\"endDate\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d1d2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SQL\n",
    "spark.sql(\n",
    "    \"select round(months_between('2019-07-01',current_date())/12,2) as years_diff\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0471e4e4",
   "metadata": {},
   "source": [
    "## time-diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0977951c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = [(\"1\", \"2019-07-01 12:01:19.111\"), (\"2\", \"2019-06-24 12:01:19.222\"),\n",
    "         (\"3\", \"2019-11-16 16:44:55.406\"), (\"4\", \"2019-11-16 16:50:59.406\")]\n",
    "\n",
    "df = spark.createDataFrame(data=dates, schema=[\"id\", \"from_timestamp\"])\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "df2=df.withColumn('from_timestamp',to_timestamp(col('from_timestamp')))\\\n",
    "  .withColumn('end_timestamp', current_timestamp())\\\n",
    "  .withColumn('DiffInSeconds',col(\"end_timestamp\").cast(\"long\") - col('from_timestamp').cast(\"long\"))\n",
    "df2.show(truncate=False)\n",
    "\n",
    "df.withColumn('from_timestamp',to_timestamp(col('from_timestamp')))\\\n",
    "  .withColumn('end_timestamp', current_timestamp())\\\n",
    "  .withColumn('DiffInSeconds',unix_timestamp(\"end_timestamp\") - unix_timestamp('from_timestamp')) \\\n",
    "  .show(truncate=False)\n",
    "\n",
    "df2.withColumn('DiffInMinutes',round(col('DiffInSeconds')/60))\\\n",
    "  .show(truncate=False)\n",
    "\n",
    "df2.withColumn('DiffInHours',round(col('DiffInSeconds')/3600))\\\n",
    "  .show(truncate=False)\n",
    "\n",
    "#Difference between two timestamps when input has just timestamp\n",
    "\n",
    "data = [(\"12:01:19.000\", \"13:01:19.000\"), (\"12:01:19.000\", \"12:02:19.000\"),\n",
    "        (\"16:44:55.406\", \"17:44:55.406\"), (\"16:50:59.406\", \"16:44:59.406\")]\n",
    "df3 = spark.createDataFrame(data=data,\n",
    "                            schema=[\"from_timestamp\", \"to_timestamp\"])\n",
    "\n",
    "df3.withColumn(\"from_timestamp\",to_timestamp(col(\"from_timestamp\"),\"HH:mm:ss.SSS\")) \\\n",
    "   .withColumn(\"to_timestamp\",to_timestamp(col(\"to_timestamp\"),\"HH:mm:ss.SSS\")) \\\n",
    "   .withColumn(\"DiffInSeconds\", col(\"from_timestamp\").cast(\"long\") - col(\"to_timestamp\").cast(\"long\")) \\\n",
    "   .withColumn(\"DiffInMinutes\",round(col(\"DiffInSeconds\")/60)) \\\n",
    "   .withColumn(\"DiffInHours\",round(col(\"DiffInSeconds\")/3600)) \\\n",
    "   .show(truncate=False)\n",
    "\n",
    "#\n",
    "\n",
    "df3 = spark.createDataFrame(data=[(\"1\", \"07-01-2019 12:01:19.406\")],\n",
    "                            schema=[\"id\", \"input_timestamp\"])\n",
    "df3.withColumn(\"input_timestamp\",to_timestamp(col(\"input_timestamp\"),\"MM-dd-yyyy HH:mm:ss.SSS\")) \\\n",
    "    .withColumn(\"current_timestamp\",current_timestamp().alias(\"current_timestamp\")) \\\n",
    "    .withColumn(\"DiffInSeconds\",current_timestamp().cast(\"long\") - col(\"input_timestamp\").cast(\"long\")) \\\n",
    "    .withColumn(\"DiffInMinutes\",round(col(\"DiffInSeconds\")/60)) \\\n",
    "    .withColumn(\"DiffInHours\",round(col(\"DiffInSeconds\")/3600)) \\\n",
    "    .withColumn(\"DiffInDays\",round(col(\"DiffInSeconds\")/24*3600)) \\\n",
    "    .show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faedeaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SQL\n",
    "spark.sql(\n",
    "    \"select unix_timestamp('2019-07-02 12:01:19') - unix_timestamp('2019-07-01 12:01:19') DiffInSeconds\"\n",
    ").show()\n",
    "spark.sql(\n",
    "    \"select (unix_timestamp('2019-07-02 12:01:19') - unix_timestamp('2019-07-01 12:01:19'))/60 DiffInMinutes\"\n",
    ").show()\n",
    "spark.sql(\n",
    "    \"select (unix_timestamp('2019-07-02 12:01:19') - unix_timestamp('2019-07-01 12:01:19'))/3600 DiffInHours\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f829cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([StructField(\"input_timestamp\", StringType(), True)])\n",
    "\n",
    "dates = [\n",
    "    '2019-07-01 12:01:19.111', '2019-06-24 12:01:19.222',\n",
    "    '2019-11-16 16:44:55.406', '2019-11-16 16:50:59.406'\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(list(zip(dates)), schema=schema)\n",
    "\n",
    "df.withColumn('input_timestamp',to_timestamp(col('input_timestamp')))\\\n",
    "  .withColumn('current_timestamp', current_timestamp().alias('current_timestamp'))\\\n",
    "  .withColumn('DiffInSeconds',current_timestamp().cast(LongType) - col('input_timestamp').cast(LongType))\\\n",
    "  .withColumn('DiffInMinutes',round(col('DiffInSeconds')/60))\\\n",
    "  .withColumn('DiffInHours',round(col('DiffInSeconds')/3600))\\\n",
    "  .withColumn('DiffInDays',round(col('DiffInSeconds')/24*3600))\\\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66360f4",
   "metadata": {},
   "source": [
    "## unix-time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26e6d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputData = [(\"2019-07-01 12:01:19\", \"07-01-2019 12:01:19\", \"07-01-2019\")]\n",
    "columns = [\"timestamp_1\", \"timestamp_2\", \"timestamp_3\"]\n",
    "df = spark.createDataFrame(data=inputData, schema=columns)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)\n",
    "\n",
    "df2 = df.select(\n",
    "    unix_timestamp(col(\"timestamp_1\")).alias(\"timestamp_1\"),\n",
    "    unix_timestamp(col(\"timestamp_2\"),\n",
    "                   \"MM-dd-yyyy HH:mm:ss\").alias(\"timestamp_2\"),\n",
    "    unix_timestamp(col(\"timestamp_3\"), \"MM-dd-yyyy\").alias(\"timestamp_3\"),\n",
    "    unix_timestamp().alias(\"timestamp_4\"))\n",
    "df2.printSchema()\n",
    "df2.show(truncate=False)\n",
    "\n",
    "df3 = df2.select(\n",
    "    from_unixtime(col(\"timestamp_1\")).alias(\"timestamp_1\"),\n",
    "    from_unixtime(col(\"timestamp_2\"),\n",
    "                  \"MM-dd-yyyy HH:mm:ss\").alias(\"timestamp_2\"),\n",
    "    from_unixtime(col(\"timestamp_3\"), \"MM-dd-yyyy\").alias(\"timestamp_3\"),\n",
    "    from_unixtime(col(\"timestamp_4\")).alias(\"timestamp_4\"))\n",
    "df3.printSchema()\n",
    "df3.show(truncate=False)\n",
    "\n",
    "#SQL"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
