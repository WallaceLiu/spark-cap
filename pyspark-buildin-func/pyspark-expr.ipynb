{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a620c8c7",
   "metadata": {},
   "source": [
    "https://sparkbyexamples.com/pyspark/pyspark-sql-expr-expression-function/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90a4cca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession.builder.appName(\"sparkbyexamples\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33e542a",
   "metadata": {},
   "source": [
    "PySpark expr() is a SQL function to execute SQL-like expressions and to use an existing DataFrame column value as an expression argument to Pyspark built-in functions. Most of the commonly used SQL functions are either part of the PySpark Column class or built-in pyspark.sql.functions API, besides these PySpark also supports many other SQL functions, so in order to use these, you have to use expr() function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e223b010",
   "metadata": {},
   "source": [
    "Below are 2 use cases of PySpark expr() funcion.\n",
    "\n",
    "- First, allowing to use of SQL-like functions that are not present in PySpark Column type & pyspark.sql.functions API. for example CASE WHEN, regr_count().\n",
    "- Second, it extends the PySpark SQL Functions by allowing to use DataFrame columns in functions for expression. for example, if you wanted to add a month value from a column to a Date column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f74465",
   "metadata": {},
   "source": [
    "# PySpark expr() Syntax\n",
    "Following is syntax of the expr() function.\n",
    "\n",
    "```python\n",
    "expr(str)\n",
    "```\n",
    "\n",
    "expr() function takes SQL expression as a string argument, executes the expression, and returns a PySpark Column type. Expressions provided with this function are not a compile-time safety like DataFrame operations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8319ed04",
   "metadata": {},
   "source": [
    "# PySpark SQL expr() Function Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59532fd3",
   "metadata": {},
   "source": [
    "## Concatenate Columns using || (similar to SQL)\n",
    "If you have SQL background, you pretty much familiar using || to concatenate values from two string columns, you can use expr() expression to do exactly same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9175acad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-----------+\n",
      "| col1| col2|       Name|\n",
      "+-----+-----+-----------+\n",
      "|James| Bond| James,Bond|\n",
      "|Scott|Varsa|Scott,Varsa|\n",
      "+-----+-----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Concatenate columns using || (sql like)\n",
    "data = [(\"James\", \"Bond\"), (\"Scott\", \"Varsa\")]\n",
    "df = spark.createDataFrame(data).toDF(\"col1\", \"col2\")\n",
    "df.withColumn(\"Name\", expr(\" col1 ||','|| col2\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5acbeda1",
   "metadata": {},
   "source": [
    "## Using SQL CASE WHEN with expr()\n",
    "PySpark doesnâ€™t have SQL Like CASE WHEN so in order to use this on PySpark DataFrame withColumn() or select(), you should use expr() function with expression as shown below.\n",
    "\n",
    "Here, I have used CASE WHEN expression on withColumn() by using expr(), this example updates an existing column gender with the derived values, M for male, F for Female, and unknown for others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b759bebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+\n",
      "|   name| gender|\n",
      "+-------+-------+\n",
      "|  James|   Male|\n",
      "|Michael| Female|\n",
      "|    Jen|unknown|\n",
      "+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "data = [(\"James\",\"M\"),(\"Michael\",\"F\"),(\"Jen\",\"\")]\n",
    "columns = [\"name\",\"gender\"]\n",
    "df = spark.createDataFrame(data = data, schema = columns)\n",
    "\n",
    "#Using CASE WHEN similar to SQL.\n",
    "from pyspark.sql.functions import expr\n",
    "df2=df.withColumn(\"gender\", expr(\"CASE WHEN gender = 'M' THEN 'Male' \" +\n",
    "           \"WHEN gender = 'F' THEN 'Female' ELSE 'unknown' END\"))\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e360f997",
   "metadata": {},
   "source": [
    "If you have any errors in the expression you will get the run time error but not during the compile time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395be582",
   "metadata": {},
   "source": [
    "## Using an Existing Column Value for Expression\n",
    "Most of the PySpark function takes constant literal values but sometimes we need to use a value from an existing column instead of a constant and this is not possible without expr() expression. The below example adds a number of months from an existing column instead of a Python constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8aa86e75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+----------+\n",
      "|      date|increment|  inc_date|\n",
      "+----------+---------+----------+\n",
      "|2019-01-23|        1|2019-02-23|\n",
      "|2019-06-24|        2|2019-08-24|\n",
      "|2019-09-20|        3|2019-12-20|\n",
      "+----------+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "\n",
    "data = [(\"2019-01-23\", 1), (\"2019-06-24\", 2), (\"2019-09-20\", 3)]\n",
    "df = spark.createDataFrame(data).toDF(\"date\", \"increment\")\n",
    "\n",
    "#Add Month value from another column\n",
    "df.select(df.date, df.increment,\n",
    "          expr(\"add_months(date,increment)\").alias(\"inc_date\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63673359",
   "metadata": {},
   "source": [
    "Note that Importing SQL functions are not required when using them with expr(). You see above add_months() is used without importing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7f3cd1",
   "metadata": {},
   "source": [
    "## Giving Column Alias along with expr()\n",
    "You can also use SQL like syntax to provide the alias name to the column expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af609fc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+----------+\n",
      "|      date|increment|  inc_date|\n",
      "+----------+---------+----------+\n",
      "|2019-01-23|        1|2019-02-23|\n",
      "|2019-06-24|        2|2019-08-24|\n",
      "|2019-09-20|        3|2019-12-20|\n",
      "+----------+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Providing alias using 'as'\n",
    "from pyspark.sql.functions import expr\n",
    "\n",
    "df.select(df.date, df.increment,\n",
    "          expr(\"\"\"add_months(date,increment) as inc_date\"\"\")).show()\n",
    "# This yields same output as above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d205ab74",
   "metadata": {},
   "source": [
    "## Case Function with expr()\n",
    "Below example converts long data type to String type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "994d1f70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- increment: long (nullable = true)\n",
      " |-- str_increment: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Using Cast() Function\n",
    "df.select(\"increment\",expr(\"cast(increment as string) as str_increment\")) \\\n",
    "  .printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac43780",
   "metadata": {},
   "source": [
    "## Arithmetic operations\n",
    "expr() is also used to provide arithmetic operations, below examples add value 5 to increment and creates a new column new_increment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69669ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-------------+\n",
      "|      date|increment|new_increment|\n",
      "+----------+---------+-------------+\n",
      "|2019-01-23|        1|            6|\n",
      "|2019-06-24|        2|            7|\n",
      "|2019-09-20|        3|            8|\n",
      "+----------+---------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Arthemetic operations\n",
    "df.select(df.date, df.increment, expr(\"increment + 5 as new_increment\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035b3168",
   "metadata": {},
   "source": [
    "## Using Filter with expr()\n",
    "Filter the DataFrame rows can done using expr() expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee35ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use expr()  to filter the rows\n",
    "from pyspark.sql.functions import expr\n",
    "\n",
    "data = [(100, 2), (200, 3000), (500, 500)]\n",
    "df = spark.createDataFrame(data).toDF(\"col1\", \"col2\")\n",
    "df.filter(expr(\"col1 == col2\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c037a95",
   "metadata": {},
   "source": [
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "\n",
    "from pyspark.sql.functions import expr\n",
    "#Concatenate columns\n",
    "data = [(\"James\", \"Bond\"), (\"Scott\", \"Varsa\")]\n",
    "df = spark.createDataFrame(data).toDF(\"col1\", \"col2\")\n",
    "df.withColumn(\"Name\", expr(\" col1 ||','|| col2\")).show()\n",
    "\n",
    "#Using CASE WHEN sql expression\n",
    "data = [(\"James\", \"M\"), (\"Michael\", \"F\"), (\"Jen\", \"\")]\n",
    "columns = [\"name\", \"gender\"]\n",
    "df = spark.createDataFrame(data=data, schema=columns)\n",
    "df2 = df.withColumn(\n",
    "    \"gender\",\n",
    "    expr(\"CASE WHEN gender = 'M' THEN 'Male' \" +\n",
    "         \"WHEN gender = 'F' THEN 'Female' ELSE 'unknown' END\"))\n",
    "df2.show()\n",
    "\n",
    "#Add months from a value of another column\n",
    "data = [(\"2019-01-23\", 1), (\"2019-06-24\", 2), (\"2019-09-20\", 3)]\n",
    "df = spark.createDataFrame(data).toDF(\"date\", \"increment\")\n",
    "df.select(df.date, df.increment,\n",
    "          expr(\"add_months(date,increment)\").alias(\"inc_date\")).show()\n",
    "\n",
    "# Providing alias using 'as'\n",
    "df.select(df.date, df.increment,\n",
    "          expr(\"\"\"add_months(date,increment) as inc_date\"\"\")).show()\n",
    "\n",
    "# Add\n",
    "df.select(df.date, df.increment, expr(\"increment + 5 as new_increment\")).show()\n",
    "\n",
    "# Using cast to convert data types\n",
    "df.select(\"increment\",expr(\"cast(increment as string) as str_increment\")) \\\n",
    "  .printSchema()\n",
    "\n",
    "#Use expr()  to filter the rows\n",
    "data = [(100, 2), (200, 3000), (500, 500)]\n",
    "df = spark.createDataFrame(data).toDF(\"col1\", \"col2\")\n",
    "df.filter(expr(\"col1 == col2\")).show()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172cc3be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
