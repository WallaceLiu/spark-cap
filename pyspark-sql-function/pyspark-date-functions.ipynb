{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "400c77c4",
   "metadata": {},
   "source": [
    "## 取当前日期 current_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69a1d8da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+\n",
      "| id|      date|\n",
      "+---+----------+\n",
      "|  0|2021-08-16|\n",
      "|  1|2021-08-16|\n",
      "|  2|2021-08-16|\n",
      "+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import current_date\n",
    "\n",
    "spark.range(3).withColumn('date', current_date()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc5ce30",
   "metadata": {},
   "source": [
    "## 获取当前日期和时间 current_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "efc94601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "| id|                date|\n",
      "+---+--------------------+\n",
      "|  0|2021-08-16 10:15:...|\n",
      "|  1|2021-08-16 10:15:...|\n",
      "|  2|2021-08-16 10:15:...|\n",
      "+---+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import current_timestamp\n",
    "\n",
    "spark.range(3).withColumn('date', current_timestamp()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e492816",
   "metadata": {},
   "source": [
    "## 日期格式转换 date_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "306280f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|      date|\n",
      "+----------+\n",
      "|04/08/2015|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import date_format\n",
    "\n",
    "df = spark.createDataFrame([('2015-04-08', )], ['a'])\n",
    "\n",
    "df.select(date_format('a', 'MM/dd/yyy').alias('date')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af387ab7",
   "metadata": {},
   "source": [
    "## 字符转日期to_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba39e5c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|      date|\n",
      "+----------+\n",
      "|1997-02-28|\n",
      "+----------+\n",
      "\n",
      "+-------------------+\n",
      "|                 dt|\n",
      "+-------------------+\n",
      "|1997-02-28 10:30:00|\n",
      "+-------------------+\n",
      "\n",
      "+-------------------+\n",
      "|                 dt|\n",
      "+-------------------+\n",
      "|1997-02-28 10:30:00|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_date, to_timestamp\n",
    "\n",
    "# 1.转日期\n",
    "df = spark.createDataFrame([('1997-02-28 10:30:00', )], ['t'])\n",
    "df.select(to_date(df.t).alias('date')).show()\n",
    "# [Row(date=datetime.date(1997, 2, 28))]\n",
    "# 2.带时间的日期\n",
    "df = spark.createDataFrame([('1997-02-28 10:30:00', )], ['t'])\n",
    "df.select(to_timestamp(df.t).alias('dt')).show()\n",
    "# [Row(dt=datetime.datetime(1997, 2, 28, 10, 30))]\n",
    "\n",
    "# 还可以指定日期格式\n",
    "df = spark.createDataFrame([('1997-02-28 10:30:00', )], ['t'])\n",
    "df.select(to_timestamp(df.t, 'yyyy-MM-dd HH:mm:ss').alias('dt')).show()\n",
    "# [Row(dt=datetime.datetime(1997, 2, 28, 10, 30))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19dfd1d5",
   "metadata": {},
   "source": [
    "## 获取日期中的年月日"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4448e799",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import year, month, dayofmonth\n",
    "\n",
    "df = spark.createDataFrame([('2015-04-08', )], ['a'])\n",
    "df.select(\n",
    "    year('a').alias('year'),\n",
    "    month('a').alias('month'),\n",
    "    dayofmonth('a').alias('day')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8880f7ef",
   "metadata": {},
   "source": [
    "## 获取时分秒"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a845d382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+------+\n",
      "|hour|minute|second|\n",
      "+----+------+------+\n",
      "|  13|     8|    15|\n",
      "+----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import hour, minute, second\n",
    "\n",
    "df = spark.createDataFrame([('2015-04-08 13:08:15', )], ['a'])\n",
    "df.select(\n",
    "    hour('a').alias('hour'),\n",
    "    minute('a').alias('minute'),\n",
    "    second('a').alias('second')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81148355",
   "metadata": {},
   "source": [
    "## 获取日期对应的季度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "580669e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|quarter|\n",
      "+-------+\n",
      "|      2|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import quarter\n",
    "\n",
    "df = spark.createDataFrame([('2015-04-08', )], ['a'])\n",
    "df.select(quarter('a').alias('quarter')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c342b94",
   "metadata": {},
   "source": [
    "## 日期加减date_add和date_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73affb43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|     d-add|     d-sub|\n",
      "+----------+----------+\n",
      "|2015-04-09|2015-04-07|\n",
      "+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import date_add, date_sub\n",
    "\n",
    "df = spark.createDataFrame([('2015-04-08', )], ['d'])\n",
    "df.select(date_add(df.d, 1).alias('d-add'),\n",
    "          date_sub(df.d, 1).alias('d-sub')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09379771",
   "metadata": {},
   "source": [
    "## add_months 月份加减"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "913a401e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|         d|\n",
      "+----------+\n",
      "|2015-05-08|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import add_months\n",
    "\n",
    "df = spark.createDataFrame([('2015-04-08', )], ['d'])\n",
    "\n",
    "df.select(add_months(df.d, 1).alias('d')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644fee52",
   "metadata": {},
   "source": [
    "## 日期差,月份差"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2902cf9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|diff|\n",
      "+----+\n",
      "|  32|\n",
      "+----+\n",
      "\n",
      "+----------+\n",
      "|    months|\n",
      "+----------+\n",
      "|3.94959677|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import datediff, months_between\n",
    "\n",
    "# 1.日期差\n",
    "df = spark.createDataFrame([('2015-04-08', '2015-05-10')], ['d1', 'd2'])\n",
    "df.select(datediff(df.d2, df.d1).alias('diff')).show()\n",
    "\n",
    "# 2.月份差\n",
    "df = spark.createDataFrame([('1997-02-28 10:30:00', '1996-10-30')], ['t', 'd'])\n",
    "df.select(months_between(df.t, df.d).alias('months')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9976825e",
   "metadata": {},
   "source": [
    "## 计算下一个日子的日期\n",
    "计算当前日期的下一个星期1,2,3,4,5,6,7的具体日子，属于实用函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0e85c8bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|      date|\n",
      "+----------+\n",
      "|2015-08-02|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import next_day\n",
    "\n",
    "# \"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\".\n",
    "df = spark.createDataFrame([('2015-07-27', )], ['d'])\n",
    "df.select(next_day(df.d, 'Sun').alias('date')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be3ae3f",
   "metadata": {},
   "source": [
    "## 本月的最后一个日期"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fddf1bd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|      date|\n",
      "+----------+\n",
      "|1997-02-28|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import last_day\n",
    "\n",
    "df = spark.createDataFrame([('1997-02-10', )], ['d'])\n",
    "df.select(last_day(df.d).alias('date')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eadb3590",
   "metadata": {},
   "source": [
    "## 另一个示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f4e6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[\"1\", \"2020-02-01\"], [\"2\", \"2019-03-01\"], [\"3\", \"2021-03-01\"]]\n",
    "df = spark.createDataFrame(data, [\"id\", \"input\"])\n",
    "df.show()\n",
    "\n",
    "#current_date()\n",
    "df.select(current_date().alias(\"current_date\")).show(1)\n",
    "\n",
    "#date_format()\n",
    "df.select(col(\"input\"),\n",
    "          date_format(col(\"input\"), \"MM-dd-yyyy\").alias(\"date_format\")).show()\n",
    "\n",
    "#to_date()\n",
    "df.select(col(\"input\"),\n",
    "          to_date(col(\"input\"), \"yyy-MM-dd\").alias(\"to_date\")).show()\n",
    "\n",
    "#datediff()\n",
    "df.select(col(\"input\"),\n",
    "          datediff(current_date(), col(\"input\")).alias(\"datediff\")).show()\n",
    "\n",
    "#months_between()\n",
    "df.select(col(\"input\"),\n",
    "          months_between(current_date(),\n",
    "                         col(\"input\")).alias(\"months_between\")).show()\n",
    "\n",
    "#trunc()\n",
    "df.select(col(\"input\"),\n",
    "          trunc(col(\"input\"), \"Month\").alias(\"Month_Trunc\"),\n",
    "          trunc(col(\"input\"), \"Year\").alias(\"Month_Year\"),\n",
    "          trunc(col(\"input\"), \"Month\").alias(\"Month_Trunc\")).show()\n",
    "\n",
    "#add_months() , date_add(), date_sub()\n",
    "\n",
    "df.select(col(\"input\"),\n",
    "          add_months(col(\"input\"), 3).alias(\"add_months\"),\n",
    "          add_months(col(\"input\"), -3).alias(\"sub_months\"),\n",
    "          date_add(col(\"input\"), 4).alias(\"date_add\"),\n",
    "          date_sub(col(\"input\"), 4).alias(\"date_sub\")).show()\n",
    "\n",
    "#\n",
    "\n",
    "df.select(col(\"input\"),\n",
    "          year(col(\"input\")).alias(\"year\"),\n",
    "          month(col(\"input\")).alias(\"month\"),\n",
    "          next_day(col(\"input\"), \"Sunday\").alias(\"next_day\"),\n",
    "          weekofyear(col(\"input\")).alias(\"weekofyear\")).show()\n",
    "\n",
    "df.select(\n",
    "    col(\"input\"),\n",
    "    dayofweek(col(\"input\")).alias(\"dayofweek\"),\n",
    "    dayofmonth(col(\"input\")).alias(\"dayofmonth\"),\n",
    "    dayofyear(col(\"input\")).alias(\"dayofyear\"),\n",
    ").show()\n",
    "\n",
    "data = [[\"1\", \"02-01-2020 11 01 19 06\"], [\"2\", \"03-01-2019 12 01 19 406\"],\n",
    "        [\"3\", \"03-01-2021 12 01 19 406\"]]\n",
    "df2 = spark.createDataFrame(data, [\"id\", \"input\"])\n",
    "df2.show(truncate=False)\n",
    "\n",
    "#current_timestamp()\n",
    "df2.select(current_timestamp().alias(\"current_timestamp\")).show(1,\n",
    "                                                                truncate=False)\n",
    "\n",
    "#to_timestamp()\n",
    "df2.select(\n",
    "    col(\"input\"),\n",
    "    to_timestamp(\n",
    "        col(\"input\"),\n",
    "        \"MM-dd-yyyy HH mm ss SSS\").alias(\"to_timestamp\")).show(truncate=False)\n",
    "\n",
    "#hour, minute,second\n",
    "data = [[\"1\", \"2020-02-01 11:01:19.06\"], [\"2\", \"2019-03-01 12:01:19.406\"],\n",
    "        [\"3\", \"2021-03-01 12:01:19.406\"]]\n",
    "df3 = spark.createDataFrame(data, [\"id\", \"input\"])\n",
    "\n",
    "df3.select(col(\"input\"),\n",
    "           hour(col(\"input\")).alias(\"hour\"),\n",
    "           minute(col(\"input\")).alias(\"minute\"),\n",
    "           second(col(\"input\")).alias(\"second\")).show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
