{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85ddd3bf",
   "metadata": {},
   "source": [
    "https://sparkbyexamples.com/pyspark/pyspark-read-json-file-into-dataframe/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9566e62f",
   "metadata": {},
   "source": [
    "PySpark SQL provides read.json(\"path\") to read a single line or multiline (multiple lines) JSON file into PySpark DataFrame and write.json(\"path\") to save or write to JSON file, In this tutorial, you will learn how to read a single file, multiple files, all files from a directory into DataFrame and writing DataFrame back to JSON file using Python example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "552c5870",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, BooleanType, DoubleType\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[1]\") \\\n",
    "    .appName(\"SparkByExamples.com\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6657630a",
   "metadata": {},
   "source": [
    "# PySpark Read JSON file into DataFrame\n",
    "Using read.json(\"path\") or read.format(\"json\").load(\"path\") you can read a JSON file into a PySpark DataFrame, these methods take a file path as an argument. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28efcc7",
   "metadata": {},
   "source": [
    "Unlike reading a CSV, By default JSON data source inferschema from an input file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e037359f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- City: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- Decommisioned: boolean (nullable = true)\n",
      " |-- EstimatedPopulation: long (nullable = true)\n",
      " |-- Lat: double (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- LocationText: string (nullable = true)\n",
      " |-- LocationType: string (nullable = true)\n",
      " |-- Long: double (nullable = true)\n",
      " |-- Notes: string (nullable = true)\n",
      " |-- RecordNumber: long (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- TaxReturnsFiled: long (nullable = true)\n",
      " |-- TotalWages: long (nullable = true)\n",
      " |-- WorldRegion: string (nullable = true)\n",
      " |-- Xaxis: double (nullable = true)\n",
      " |-- Yaxis: double (nullable = true)\n",
      " |-- Zaxis: double (nullable = true)\n",
      " |-- ZipCodeType: string (nullable = true)\n",
      " |-- Zipcode: long (nullable = true)\n",
      "\n",
      "+-------------------+-------+-------------+-------------------+-----+--------------------+--------------------+--------------+------+-----+------------+-----+---------------+----------+-----------+-----+-----+-----+-----------+-------+\n",
      "|               City|Country|Decommisioned|EstimatedPopulation|  Lat|            Location|        LocationText|  LocationType|  Long|Notes|RecordNumber|State|TaxReturnsFiled|TotalWages|WorldRegion|Xaxis|Yaxis|Zaxis|ZipCodeType|Zipcode|\n",
      "+-------------------+-------+-------------+-------------------+-----+--------------------+--------------------+--------------+------+-----+------------+-----+---------------+----------+-----------+-----+-----+-----+-----------+-------+\n",
      "|        PARC PARQUE|     US|        false|               null|17.96|NA-US-PR-PARC PARQUE|     Parc Parque, PR|NOT ACCEPTABLE|-66.22| null|           1|   PR|           null|      null|         NA| 0.38|-0.87|  0.3|   STANDARD|    704|\n",
      "|PASEO COSTA DEL SUR|     US|        false|               null|17.96|NA-US-PR-PASEO CO...|Paseo Costa Del S...|NOT ACCEPTABLE|-66.22| null|           2|   PR|           null|      null|         NA| 0.38|-0.87|  0.3|   STANDARD|    704|\n",
      "+-------------------+-------+-------------+-------------------+-----+--------------------+--------------------+--------------+------+-----+------------+-----+---------------+----------+-----------+-----+-----+-----+-----------+-------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read JSON file into dataframe\n",
    "df = spark.read.json(\"../data/resources/zipcodes.json\")\n",
    "df.printSchema()\n",
    "df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "778765c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read JSON file into dataframe\n",
    "df = spark.read.format('org.apache.spark.sql.json') \\\n",
    "        .load(\"../data/resources/zipcodes.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e88d8fb",
   "metadata": {},
   "source": [
    "# Read JSON file from multiline\n",
    "PySpark JSON data source provides multiple options to read files in different options, use multiline option to read JSON files scattered across multiple lines. By default multiline option, is set to false."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba3a8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read multiline json file\n",
    "multiline_df = spark.read.option(\"multiline\",\"true\") \\\n",
    "      .json(\"../data/resources/multiline-zipcode.json\")\n",
    "multiline_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af52f462",
   "metadata": {},
   "source": [
    "# Reading multiple files at a time\n",
    "Using the read.json() method you can also read multiple JSON files from different paths, just pass all file names with fully qualified paths by separating comma, for example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d70eae94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------+-------------+-----+--------------------+--------------------+--------------+------+------------+-----+-----------+-----+-----+-----+-----------+-------+\n",
      "|               City|Country|Decommisioned|  Lat|            Location|        LocationText|  LocationType|  Long|RecordNumber|State|WorldRegion|Xaxis|Yaxis|Zaxis|ZipCodeType|Zipcode|\n",
      "+-------------------+-------+-------------+-----+--------------------+--------------------+--------------+------+------------+-----+-----------+-----+-----+-----+-----------+-------+\n",
      "|PASEO COSTA DEL SUR|     US|        false|17.96|NA-US-PR-PASEO CO...|Paseo Costa Del S...|NOT ACCEPTABLE|-66.22|           2|   PR|         NA| 0.38|-0.87|  0.3|   STANDARD|    704|\n",
      "|       BDA SAN LUIS|     US|        false|18.14|NA-US-PR-BDA SAN ...|    Bda San Luis, PR|NOT ACCEPTABLE|-66.26|          10|   PR|         NA| 0.38|-0.86| 0.31|   STANDARD|    709|\n",
      "|        PARC PARQUE|     US|        false|17.96|NA-US-PR-PARC PARQUE|     Parc Parque, PR|NOT ACCEPTABLE|-66.22|           1|   PR|         NA| 0.38|-0.87|  0.3|   STANDARD|    704|\n",
      "+-------------------+-------+-------------+-----+--------------------+--------------------+--------------+------+------------+-----+-----------+-----+-----+-----+-----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read multiple files\n",
    "df2 = spark.read.json(['../data/resources/zipcode2.json', '../data/resources/zipcode1.json'])\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c78705",
   "metadata": {},
   "source": [
    "# Reading all files in a directory\n",
    "We can read all JSON files from a directory into DataFrame just by passing directory as a path to the json() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e53e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read All JSON files from a directory\n",
    "df3 = spark.read.json(\"../data/resources/*.json\")\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e96ac9",
   "metadata": {},
   "source": [
    "# Reading files with a user-specified custom schema\n",
    "PySpark Schema defines the structure of the data, in other words, it is the structure of the DataFrame. PySpark SQL provides StructType & StructField classes to programmatically specify the structure to the DataFrame.\n",
    "\n",
    "If you know the schema of the file ahead and do not want to use the default inferSchema option, use schema option to specify user-defined custom column names and data types.\n",
    "\n",
    "Use the PySpark StructType class to create a custom schema, below we initiate this class and use add a method to add columns to it by providing the column name, data type and nullable option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88c8645e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- RecordNumber: integer (nullable = true)\n",
      " |-- Zipcode: integer (nullable = true)\n",
      " |-- ZipCodeType: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- LocationType: string (nullable = true)\n",
      " |-- Lat: double (nullable = true)\n",
      " |-- Long: double (nullable = true)\n",
      " |-- Xaxis: integer (nullable = true)\n",
      " |-- Yaxis: double (nullable = true)\n",
      " |-- Zaxis: double (nullable = true)\n",
      " |-- WorldRegion: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- LocationText: string (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- Decommisioned: boolean (nullable = true)\n",
      " |-- TaxReturnsFiled: string (nullable = true)\n",
      " |-- EstimatedPopulation: integer (nullable = true)\n",
      " |-- TotalWages: integer (nullable = true)\n",
      " |-- Notes: string (nullable = true)\n",
      "\n",
      "+------------+-------+-----------+-------------------+-----+--------------+-----+-------+-----+-----+-----+-----------+-------+--------------------+--------------------+-------------+---------------+-------------------+----------+-------------+\n",
      "|RecordNumber|Zipcode|ZipCodeType|               City|State|  LocationType|  Lat|   Long|Xaxis|Yaxis|Zaxis|WorldRegion|Country|        LocationText|            Location|Decommisioned|TaxReturnsFiled|EstimatedPopulation|TotalWages|        Notes|\n",
      "+------------+-------+-----------+-------------------+-----+--------------+-----+-------+-----+-----+-----+-----------+-------+--------------------+--------------------+-------------+---------------+-------------------+----------+-------------+\n",
      "|           1|    704|   STANDARD|        PARC PARQUE|   PR|NOT ACCEPTABLE|17.96| -66.22| null|-0.87|  0.3|         NA|     US|     Parc Parque, PR|NA-US-PR-PARC PARQUE|        false|           null|               null|      null|         null|\n",
      "|           2|    704|   STANDARD|PASEO COSTA DEL SUR|   PR|NOT ACCEPTABLE|17.96| -66.22| null|-0.87|  0.3|         NA|     US|Paseo Costa Del S...|NA-US-PR-PASEO CO...|        false|           null|               null|      null|         null|\n",
      "|          10|    709|   STANDARD|       BDA SAN LUIS|   PR|NOT ACCEPTABLE|18.14| -66.26| null|-0.86| 0.31|         NA|     US|    Bda San Luis, PR|NA-US-PR-BDA SAN ...|        false|           null|               null|      null|         null|\n",
      "|       61391|  76166|     UNIQUE|  CINGULAR WIRELESS|   TX|NOT ACCEPTABLE|32.72| -97.31| null|-0.83| 0.54|         NA|     US|Cingular Wireless...|NA-US-TX-CINGULAR...|        false|           null|               null|      null|         null|\n",
      "|       61392|  76177|   STANDARD|         FORT WORTH|   TX|       PRIMARY|32.75| -97.33| null|-0.83| 0.54|         NA|     US|      Fort Worth, TX| NA-US-TX-FORT WORTH|        false|           2126|               4053| 122396986|         null|\n",
      "|       61393|  76177|   STANDARD|           FT WORTH|   TX|    ACCEPTABLE|32.75| -97.33| null|-0.83| 0.54|         NA|     US|        Ft Worth, TX|   NA-US-TX-FT WORTH|        false|           2126|               4053| 122396986|         null|\n",
      "|           4|    704|   STANDARD|    URB EUGENE RICE|   PR|NOT ACCEPTABLE|17.96| -66.22| null|-0.87|  0.3|         NA|     US| Urb Eugene Rice, PR|NA-US-PR-URB EUGE...|        false|           null|               null|      null|         null|\n",
      "|       39827|  85209|   STANDARD|               MESA|   AZ|       PRIMARY|33.37|-111.64| null|-0.77| 0.55|         NA|     US|            Mesa, AZ|       NA-US-AZ-MESA|        false|          14962|              26883| 563792730|no NWS data, |\n",
      "|       39828|  85210|   STANDARD|               MESA|   AZ|       PRIMARY|33.38|-111.84| null|-0.77| 0.55|         NA|     US|            Mesa, AZ|       NA-US-AZ-MESA|        false|          14374|              25446| 471000465|         null|\n",
      "|       49345|  32046|   STANDARD|           HILLIARD|   FL|       PRIMARY|30.69| -81.92| null|-0.85| 0.51|         NA|     US|        Hilliard, FL|   NA-US-FL-HILLIARD|        false|           3922|               7443| 133112149|         null|\n",
      "|       49346|  34445|     PO BOX|             HOLDER|   FL|       PRIMARY|28.96| -82.41| null|-0.86| 0.48|         NA|     US|          Holder, FL|     NA-US-FL-HOLDER|        false|           null|               null|      null|         null|\n",
      "|       49347|  32564|   STANDARD|               HOLT|   FL|       PRIMARY|30.72| -86.67| null|-0.85| 0.51|         NA|     US|            Holt, FL|       NA-US-FL-HOLT|        false|           1207|               2190|  36395913|         null|\n",
      "|       49348|  34487|     PO BOX|          HOMOSASSA|   FL|       PRIMARY|28.78| -82.61| null|-0.86| 0.48|         NA|     US|       Homosassa, FL|  NA-US-FL-HOMOSASSA|        false|           null|               null|      null|         null|\n",
      "|          10|    708|   STANDARD|       BDA SAN LUIS|   PR|NOT ACCEPTABLE|18.14| -66.26| null|-0.86| 0.31|         NA|     US|    Bda San Luis, PR|NA-US-PR-BDA SAN ...|        false|           null|               null|      null|         null|\n",
      "|           3|    704|   STANDARD|      SECT LANAUSSE|   PR|NOT ACCEPTABLE|17.96| -66.22| null|-0.87|  0.3|         NA|     US|   Sect Lanausse, PR|NA-US-PR-SECT LAN...|        false|           null|               null|      null|         null|\n",
      "|       54354|  36275|     PO BOX|      SPRING GARDEN|   AL|       PRIMARY|33.97| -85.55| null|-0.82| 0.55|         NA|     US|   Spring Garden, AL|NA-US-AL-SPRING G...|        false|           null|               null|      null|         null|\n",
      "|       54355|  35146|   STANDARD|        SPRINGVILLE|   AL|       PRIMARY|33.77| -86.47| null|-0.82| 0.55|         NA|     US|     Springville, AL|NA-US-AL-SPRINGVILLE|        false|           4046|               7845| 172127599|         null|\n",
      "|       54356|  35585|   STANDARD|        SPRUCE PINE|   AL|       PRIMARY|34.37| -87.69| null|-0.82| 0.56|         NA|     US|     Spruce Pine, AL|NA-US-AL-SPRUCE PINE|        false|            610|               1209|  18525517|         null|\n",
      "|       76511|  27007|   STANDARD|           ASH HILL|   NC|NOT ACCEPTABLE| 36.4| -80.56| null|-0.79| 0.59|         NA|     US|        Ash Hill, NC|   NA-US-NC-ASH HILL|        false|            842|               1666|  28876493|         null|\n",
      "|       76512|  27203|   STANDARD|           ASHEBORO|   NC|       PRIMARY|35.71| -79.81| null|-0.79| 0.58|         NA|     US|        Asheboro, NC|   NA-US-NC-ASHEBORO|        false|           8355|              15228| 215474318|         null|\n",
      "+------------+-------+-----------+-------------------+-----+--------------+-----+-------+-----+-----+-----+-----------+-------+--------------------+--------------------+-------------+---------------+-------------------+----------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define custom schema\n",
    "schema = StructType([\n",
    "    StructField(\"RecordNumber\", IntegerType(), True),\n",
    "    StructField(\"Zipcode\", IntegerType(), True),\n",
    "    StructField(\"ZipCodeType\", StringType(), True),\n",
    "    StructField(\"City\", StringType(), True),\n",
    "    StructField(\"State\", StringType(), True),\n",
    "    StructField(\"LocationType\", StringType(), True),\n",
    "    StructField(\"Lat\", DoubleType(), True),\n",
    "    StructField(\"Long\", DoubleType(), True),\n",
    "    StructField(\"Xaxis\", IntegerType(), True),\n",
    "    StructField(\"Yaxis\", DoubleType(), True),\n",
    "    StructField(\"Zaxis\", DoubleType(), True),\n",
    "    StructField(\"WorldRegion\", StringType(), True),\n",
    "    StructField(\"Country\", StringType(), True),\n",
    "    StructField(\"LocationText\", StringType(), True),\n",
    "    StructField(\"Location\", StringType(), True),\n",
    "    StructField(\"Decommisioned\", BooleanType(), True),\n",
    "    StructField(\"TaxReturnsFiled\", StringType(), True),\n",
    "    StructField(\"EstimatedPopulation\", IntegerType(), True),\n",
    "    StructField(\"TotalWages\", IntegerType(), True),\n",
    "    StructField(\"Notes\", StringType(), True)\n",
    "])\n",
    "\n",
    "df_with_schema = spark.read.schema(schema) \\\n",
    "        .json(\"../data/resources/zipcodes.json\")\n",
    "df_with_schema.printSchema()\n",
    "df_with_schema.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df8b914",
   "metadata": {},
   "source": [
    "# Read JSON file using PySpark SQL\n",
    "PySpark SQL also provides a way to read a JSON file by creating a temporary view directly from the reading file using spark.sqlContext.sql(“load JSON to temporary view”)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a01383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a table from Parquet File\n",
    "spark.sql(\"CREATE OR REPLACE TEMPORARY VIEW zipcode3 USING json OPTIONS\" +\n",
    "          \" (path '../data/resources/zipcodes.json')\")\n",
    "spark.sql(\"select * from zipcode3\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52436c0",
   "metadata": {},
   "source": [
    "# Options while reading JSON file\n",
    "## nullValues\n",
    "Using nullValues option you can specify the string in a JSON to consider as null. For example, if you want to consider a date column with a value “1900-01-01” set null on DataFrame.\n",
    "\n",
    "## dateFormat\n",
    "dateFormat option to used to set the format of the input DateType and TimestampType columns. Supports all java.text.SimpleDateFormat formats.\n",
    "\n",
    "Note: Besides the above options, PySpark JSON dataset also supports many other options."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d22c669",
   "metadata": {},
   "source": [
    "# Applying DataFrame transformations\n",
    "Once you have create PySpark DataFrame from the JSON file, you can apply all transformation and actions DataFrame support. Please refer to the link for more details. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f6f527",
   "metadata": {},
   "source": [
    "# Write PySpark DataFrame to JSON file\n",
    "Use the PySpark DataFrameWriter object “write” method on DataFrame to write a JSON file. \n",
    "\n",
    "```python\n",
    "df2.write.json(\"/tmp/spark_output/zipcodes.json\")\n",
    "```\n",
    "\n",
    "## PySpark Options while writing JSON files\n",
    "While writing a JSON file you can use several options.  \n",
    "\n",
    "Other options available nullValue,dateFormat\n",
    "\n",
    "## PySpark Saving modes\n",
    "PySpark DataFrameWriter also has a method mode() to specify SaveMode; the argument to this method either takes overwrite, append, ignore, errorifexists.\n",
    "\n",
    "- overwrite – mode is used to overwrite the existing file\n",
    "- append – To add the data to the existing file\n",
    "- ignore – Ignores write operation when the file already exists\n",
    "- errorifexists or error – This is a default option when the file already exists, it returns an error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1c1719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PySpark write Parquet File\n",
    "df2.write.mode('Overwrite').json(\"/tmp/spark_output/zipcodes.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
